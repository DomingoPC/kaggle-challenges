---
title: "Calory Lost Prediction"
format: html
editor: visual
---

## Initialization

```{r}
library(tidyverse)
library(glmnet)
library(mgcv)
```

```{r}
# Load data
df <- read.csv(file = 'data/train.csv') %>% select(-id)
head(df) # also shows types
print(dim(df))
```

Submission file:
```{r}
# Read file
df.submission <- read.csv(file = 'data/test.csv')

# Store ids and remove from data frame
submissions_ids <- df.submission$id
df.submission <- df.submission %>% select(-id)
```


```{r}
# Custom Evaluation Metric
RMSLE <- function(y_real, y_pred) {
  N <- length(y_real) # number of cases (real ones)
  
  # Check if both are the same length
  if (N != length(y_pred)) {
    print('Error: predictions and real values are not the same length')
  } else {
    # Compute summation
    summation <- 0
    for (i in c(1:N)) {
      summation <- summation + (log(1 + y_pred[[i]]) - log(1 + y_real[[i]]))^2
    }
    
    # Compute RMSLE
    return(
      sqrt(1/N * summation)
    )
  }
}
```

```{r}
# Train+validation and test samples sizes
sample_size_train_val <- floor(0.8 * nrow(df))
sample_size_test <- floor(0.2 * nrow(df))

# Get indices: train and test
idx_train_val <- sort(sample(seq_len(nrow(df)), size = sample_size_train_val))
idx_test <- setdiff(seq_len(nrow(df)), idx_train_val)

# Extract validation subset from train
idx_validation <- sort(
  sample(idx_train_val, size = sample_size_test) # same size as test
) 

# Update train, so it does not include validation
idx_train <- setdiff(idx_train_val, idx_validation)

# Create three dataframes: training, validation and test
df.train <- df[idx_train, ]
df.val <- df[idx_validation, ]
df.test <- df[idx_test, ]
```

## Cleaning and Preprocessing

```{r}
# Missing values
missing_values <- as.data.frame(colSums(is.na(df)))
colnames(missing_values) <- 'Number of Missing Values'
missing_values
```

```{r}
# Remove Duplicates
cat(paste(
  'Initial Data Frame Shape = ', '(', nrow(df), ', ', ncol(df), ')', '.',
  collapse='', sep = ''
  )
)
cat('\n')

df = distinct(df)

cat(paste(
  'Final Data Frame Shape = ', '(', nrow(df), ', ', ncol(df), ')', '.',
  collapse='', sep = ''
  )
)
```

## EDA

### Univariate Analysis

```{r}
df %>% 
  select_if(is.numeric) %>% 
  apply(2, summary)
```

-   Workouts with a caloric loss of 1.0 seems strange.

```{r}
# Is the data balanced in 'Sex'?
df %>% 
  select('Sex') %>% 
  table() %>% 
  prop.table()
```

-   It is balanced.

```{r}
# Outliers
df %>% 
  pivot_longer(cols = -'Sex', names_to = 'Variable', values_to = 'Value') %>% 
  ggplot(aes(x=Value, y=Variable, fill=Sex, color=Sex)) +
  geom_boxplot() +
  scale_fill_manual(values=alpha(c('salmon', 'skyblue'), .3))
```

-   Body_Temp has little variability.

-   Height and Weight show the biggest amount of outliers.

```{r}
# Get numerical variables names
col_names <- df %>% 
  select_if(is.numeric) %>% 
  colnames()

# Plot all numerical variables
for (col_name in col_names) {
  # Title label text
  title <- paste(col_name, 'histogram and density function splitted by Sex')
  
  # Histogram and Density Plot by 'Sex'
  plt <- df %>% 
    ggplot(aes(x=.data[[col_name]])) +
    geom_histogram(aes(y=..density..), fill='black', alpha=.4) +
    geom_density(aes(color=Sex), size=1.2) +
    labs(title=title, x=col_name) +
    theme_minimal() 
  print(plt)
}

```

-   Mostly the same, except for 'Weight', 'Height', 'Body_Temp' (slightly) and 'Calories'.

    -   **Weight and Height:** Men tend to be taller and heavier than women.

    -   **Calories:** Women tend to lose an intermediate amount of calories, while men populate the highest and lowest values more.

    -   **Body_Temp:** There seems to be a slightly higher amount of women at higher body temperatures than men.

-   Age, Body_Temp and Calories show skewness, and Height and Weight don't, but they are best centred separated by Sex.

### Bivariate Analysis

```{r}
# Correlation matrix
corr <- cor(df %>% select_if(is.numeric))
as.data.frame(head(corr))
```

```{r}
# Plot correlation matrix
corrplot::corrplot(corr, method='shade', order='hclust', addrect=2)
```

-   Calories is heavily related to the duration of the workout, and by the heart rate and body temperature during it.

-   Height, weight and age do not correlate well with the target variable.

-   Height and weight are heavily correlated, same with body temperature and duration.

-   Age does not correlate well with any other variable. It shows that the scope by age is wide, but also that the impact in calories may not be determined by it.

```{r}
name_pairs <- list(
  c('Height', 'Weight'), 
  c('Body_Temp', 'Duration'),
  c('Body_Temp', 'Heart_Rate'),
  c('Body_Temp', 'Calories'),
  c('Heart_Rate', 'Calories'),
  c('Duration', 'Calories')
)

# Scatter plots
for (name_vec in name_pairs) {
  # Get names
  name1 <- name_vec[[1]]
  name2 <- name_vec[[2]]
  
  # Plot
  plt <- df %>% 
    sample_n(1e4) %>% 
    ggplot(aes(x=.data[[name1]], y=.data[[name2]])) + 
    geom_point(alpha = 0.2, size=1)
  print(plt)
}

```

-   Exponential relationship between Body_temp and Calories. Transforming Body_Temp should improve predictions, although it could obtain a higher correlation with duration as well.

-   Linear relationship between Heart_Rate and Calories with stable variability.

-   Approximately linear relationship between Duration and Calories, but data variability increases with Duration.

### Multivariate Analysis

```{r}
# Extract p-value from linear model
m.lm <- lm('Calories ~ .', df.train)
summary(m.lm)

# Print custom evaluation metric
pred <- as.vector(predict(m.lm, df.test))
pred[pred<0] <- 0 # Remove negative calories prediction
print(paste('RMSLE =', round(RMSLE(df.test$Calories, pred), 4)))
```

-   As of 28/05/2025 the best has a RMSLE score of 0.05624.

## Feature Engineering and Variable Selection

### Box-Cox Transform: Reduce Skewness

First, let's perform Anderson-Darling Tests to discover which variables are mathematically skewed.

```{r}
# Anderson-Darling Test 
library(nortest)  

test_normality <- function(x) {   
  # Ensure complete data   
  if ( all(!is.na(x)) && length(unique(x)) > 1 ) { 
    # Perform Kolmogorov-Smirnov Test     
    return( ad.test(x)$p.value )   
  } else {     
    return(NA)   
  } 
}  

# Apply to all numeric columns (except target variable) 
normality <- df.train %>%    
  summarise(across(     
    where(is.numeric) & !Calories, test_normality
  )) %>%    
  pivot_longer(everything(), names_to = 'Variable', values_to = 'p_value')

# Identify variables that need transformation (p-value < 0.05)
vars_need_transform <- normality %>% 
  filter(p_value < 0.05 & !is.na(p_value)) %>% 
  pull(Variable)

cat(paste0(
  'Variables needing transformation:\n',
  paste(vars_need_transform, collapse = ', '),
  '.'
))
```

```{r}
# Visual assessment
df.train %>% 
  select(where(is.numeric), -Calories) %>% 
  sample_n(1e3) %>% 
  pivot_longer(everything(), names_to = 'Variable', values_to = 'Value') %>% 
  
  ggplot(aes(sample = Value)) +
  stat_qq() + 
  stat_qq_line(color = 'red') +
  facet_wrap(~Variable, scales = 'free') +
  
  theme_minimal() + 
  labs(title = 'Q-Q Plots for Normality Assessment')
```

Example with Body_Temp:

```{r}
library(forecast)

# Box-Cox transformation of Body_Temp
var_name <- 'Body_Temp'
var_name2 <- paste0(var_name, '_transformed')

lambda <- BoxCox.lambda(df.train[[var_name]] %>% sample(size=1e5))
df.train_trans <- BoxCox(df.train[[var_name]], lambda)

# Compare boxcox transformation (plot)
compare_transformation <- data.frame(
  df.train[var_name],
  df.train_trans
)

colnames(compare_transformation) <- c(var_name, var_name2)

# Histograms
compare_transformation %>% 
  ggplot(aes(y=..density..)) +
  geom_histogram(aes(x=.data[[var_name]]), fill='black', alpha=.4) +
  geom_histogram(aes(x=.data[[var_name2]]), fill='salmon', alpha=.4)

# Q-Q Plot
compare_transformation %>% 
  sample_n(1e3) %>% 
  pivot_longer(everything(), names_to = 'Variable', values_to = 'Value') %>% 
  ggplot(aes(sample = Value)) +
  stat_qq() +
  stat_qq_line(color = 'red') +
  facet_wrap(~Variable, scale = 'free')
```

Apply to all variables that need it:

```{r}
# Transform
df.train_trans <- df.train
lambda <- list()

for (var in vars_need_transform) {
  lambda[var] <- BoxCox.lambda(df.train[[var]] %>% sample(1e4))
  df.train_trans[[var]] <- BoxCox(df.train[[var]], lambda[[var]])
}

df.train_trans %>% 
  select(where(is.numeric), -Calories) %>% 
  sample_n(1e3) %>% 
  pivot_longer(everything(), names_to = 'Variable', values_to = 'Value') %>% 
  
  ggplot(aes(sample = Value)) +
  stat_qq() + 
  stat_qq_line(color = 'red') +
  facet_wrap(~Variable, scales = 'free') +
  
  theme_minimal() + 
  labs(title = 'Q-Q Plots for Normality Assessment')
```

```{r}
# Store transformation
df.train[, vars_need_transform] <- df.train_trans[, vars_need_transform]
```

### Standardization

```{r}
# Get numeric variables (but calories)
numeric_vars <- setdiff(
  df.train %>% select_if(is.numeric) %>% names(),
  'Calories'
)

# Get original means and standard deviation for scaling
numeric_mean_sd <- df.train %>% 
  select(numeric_vars) %>% 
  gather(key = 'variable', value = 'value', factor_key = T) %>% 
  group_by(variable) %>% 
  summarise(mean = mean(value), sd = sd(value), .groups = 'drop')

# Scale train: (x - mean) / sd
df.train <- df.train %>% 
  mutate(
    across(
      all_of(numeric_vars),
      # Define anonymous function with instructions
      ~{
        # Get current column name accesed by across
        col_name <- cur_column()
        
        # Extract mean and sd for this specific column
        col_stats <- numeric_mean_sd %>% 
          filter(variable == col_name)
        
        col_mean <- col_stats$mean
        col_sd <- col_stats$sd
        
        # Apply scaling transformation
        # .x is the current column being processed
        (.x - col_mean) / col_sd
      }
    )
  )

head(df.train)
```

### Variable Transforms

Body_Temp:

```{r}
# Exponentiate Body_Temp
df.train['exp_Body_Temp'] <- exp(df.train$Body_Temp)
df.val['exp_Body_Temp'] <- exp(df.val$Body_Temp)

# Plot against Calories
df.train %>% 
  sample_n(1e4) %>% 
  ggplot() +
  # geom_point(aes(x=Body_Temp, y=Calories), alpha=.3) +
  geom_point(aes(x=exp_Body_Temp, y=Calories), alpha=.3, color='red') 
```

### Clustering: KMeans

```{r}
m.kmeans <- kmeans(df.train %>% select_if(is.numeric), centers = 4, nstart = 25)
```

```{r}
df.train$cluster <- as.factor(m.kmeans$cluster)
table(df.train$cluster)
```

```{r}
# 2. For test data, predict cluster based on features only
predict_test_clusters <- function(df_test, kmeans_model) {
  # Find closest cluster centre (using only feature dimensions)
  feature_centres <- subset(m.kmeans$centers, select = -Calories)
  centre_names <- colnames(feature_centres)
  
  # Test data frame
  test_features <- df_test %>% select(centre_names) # Exclude target variable
  
  # Calculate distance to every centre
  distances <- apply(
    test_features, 1, function(row) { # get every row
      apply(feature_centres, 1, function(centre) { # get every centre
        # Calculate Euclidean distance
        sqrt(sum((row - centre)^2))
      })
    }
  )
  
  # Select the cluster closest to the specific row
  clusters <- apply(distances, 2, which.min)
  return(as.factor(clusters))
}

df.val$cluster <- predict_test_clusters(df.val, m.kmeans)
head(df.val)
```

### Try lm again

```{r}
data <- df.train 
data$Sex <- as.factor(data$Sex)

# Extract p-value from linear model
m.lm <- lm('Calories ~ .', data)
summary(m.lm)

# Print custom evaluation metric
pred <- as.vector(predict(m.lm, df.val))
pred[pred<0] <- 0 # Remove negative calories prediction
print(paste('RMSLE =', round(RMSLE(df.val$Calories, pred), 4)))
```

-   Building cluster with a hybrid approach and using exp_Body_Temp has lowered RMSLE from \~0.5 to \~0.4.

### Final Pipeline (apply to validation and test)

```{r}
pipeline <- function(data) {
  # --- Box-Cox transform ---
  for (var in vars_need_transform) {
    data[var] <- BoxCox(data[[var]], lambda[[var]])
  }
  
  # --- Scaling from numeric_mean_sd (original mean and sd) ---
  # Get named vector of means and standard deviations
  means_vec <- setNames(object = numeric_mean_sd$mean, 
                        nm = numeric_mean_sd$variable)
  sds_vec <- setNames(object = numeric_mean_sd$sd, 
                        nm = numeric_mean_sd$variable)
  
  # Scale the data using the named vectors
  data <- data %>% 
    mutate(
      across(
        all_of(numeric_vars),
        ~( (.x - means_vec[cur_column()]) / sds_vec[cur_column()] )
      )
    )
  
  # --- Exponentiation of Body_Temp ---
  data$exp_Body_Temp <- exp(data$Body_Temp)
  
  # --- KMeans Clustering ---
  # For test and validation: predict cluster based on distance to centres
  # (because those cannot access their "calories" values)
  data$cluster <- predict_test_clusters(data, m.kmeans)
  
  return(data)
}

df.val <- pipeline(df.val)
df.test <- pipeline(df.test)
df.submission <- pipeline(df.submission)
head(df.test)
```

## Modeling

```{r}
evaluate <- function(model, rmv.neg = TRUE) {
  # Predictions
  pred_val <- as.vector(predict(model, df.val))
  pred_test <- as.vector(predict(model, df.test))
  
  # Remove negative calories prediction
  if (rmv.neg) {
    pred_val[pred_val<0] <- 0
    pred_test[pred_test<0] <- 0
  }
  
  # Print results
  print(paste('Validation: RMSLE =', round(RMSLE(df.val$Calories, pred_val), 4)))
  print(paste('Test: RMSLE =', round(RMSLE(df.test$Calories, pred_test), 4)))
}
```

### Linear Model

```{r}
data <- df.train 
data$Sex <- as.factor(data$Sex)

# Extract p-value from linear model
m.lm <- lm('Calories ~ .', data)
summary(m.lm)

# Print custom evaluation metric
evaluate(m.lm)
```

### Generalised Linear Model

```{r}
m.glm <- glm('Calories ~ .', df.train, family = 'poisson')
summary(m.glm)
evaluate(m.glm)

```

### Regularization

```{r}
target <- 'Calories'
indep <- setdiff(colnames(df.train), c('Calories', 'Sex'))
X <- as.matrix(df.train[, indep])
y <- df.train[, target]

# Find best parameters with cross-validation
cv_model <- cv.glmnet(x=X, y=y, alpha = 0.5)  # 0.5 = elastic net

# Fit final model
model <- glmnet(X, y, alpha = 0.5, lambda = cv_model$lambda.min)
```

```{r}
pred <- predict(model, newx = as.matrix(df.val[, indep]))
pred[pred<0] <- 0
RMSLE(df.val$Calories, pred)

coef(model)
```

+ The main variables are: Duration, Heart_Rate, Body_Temp and exp_Body_Temp. The rest don't contribute as much

+ All variables are kept, indicating all of them are useful.

+ The change in coefficient sign in Body_Temp and exp_Body_Temp is interesting, but it might be caused by exp_Body_Temp being strictly positive and the elastic-net model itself — trying to balance the effeects of Body_Temp.


## Feature Engineering: after checking variable importance

```{r}
feat.eng <- function(data) {
  # Interaction terms for important variables
  data$Duration_HeartRate <- data$Duration * data$Heart_Rate
  data$Duration_expBodyTemp <- data$Duration * data$exp_Body_Temp
  data$HeartRate_expBodyTemp <- data$Heart_Rate * data$exp_Body_Temp
  
  # Other transformations of body temperature
  data$Body_Temp_sq <- data$Body_Temp ^ 2
  
  data
}

df.train <- feat.eng(df.train)
df.val <- feat.eng(df.val)
df.test <- feat.eng(df.test)
df.submission <- feat.eng(df.submission)
head(df.train)
```

Update clusters:
```{r}
m.kmeans <- kmeans(df.train %>% select_if(is.numeric), centers = 4, nstart = 25)

# Cluster distribution
df.train$cluster <- as.factor(m.kmeans$cluster)
table(df.train$cluster)
```

```{r}
# Update clusters
df.val$cluster <- predict_test_clusters(df.val, m.kmeans)
df.test$cluster <- predict_test_clusters(df.test, m.kmeans)
df.submission$cluster <- predict_test_clusters(df.submission, m.kmeans)
head(df.val)
```


```{r}
# Regularization
target <- 'Calories'
indep <- setdiff(colnames(df.train), c('Calories', 'Sex'))
X <- as.matrix(df.train[, indep])
y <- df.train[, target]

# Find best parameters with cross-validation
cv_model <- cv.glmnet(x=X, y=y, alpha = 0.5)  # 0.5 = elastic net

# Fit final model
model <- glmnet(X, y, alpha = 0.5, lambda = cv_model$lambda.min)

# Prediction
pred <- predict(model, newx = as.matrix(df.val[, indep]))
pred[pred<0] <- 0

# Evaluation
print(RMSLE(df.val$Calories, pred))

# Coefficients
coef(model)
```

Linear Model:
```{r}
data <- df.train 
data$Sex <- as.factor(data$Sex)

# Extract p-value from linear model
m.lm_2 <- lm('Calories ~ . - Body_Temp', data)
summary(m.lm_2)

# Print custom evaluation metric
evaluate(m.lm_2)
```

BAM model (more efficient than GAM):
```{r}
m.bam <- bam(
  Calories ~ s(Duration) + s(Heart_Rate) + s(Body_Temp),
  data = df.train,
  method = 'fREML' #fast REML
)

summary(m.bam)

# Print custom evaluation metric
cat('\n')
evaluate(m.bam)
```

+ A high F score — extremely high for that matter —, means that the variables where non-linear. For instance, the score in s(Duration) can be understood like this variable explains 60717 times more variance than random noise, meaning that they are not just flat lines of noise.

+ Some robustness is shown as well, due to the validation and test RMSLE being similar (and low).

```{r}
# Visualize smooth functions
plot(m.bam, pages = 1, shade = TRUE)
```

```{r}
# Check for any remaining patterns
gam.check(m.bam)
```

+ Residuals mostly look like a random cloud of points around zero (good sign!).

+ k-index are close to one and the p-values (indicating if k-index is too small) discard the null hypothesis (also good news).

+ Duration has a slightly lower than 1.0 k-index, which indicates that it might benefit from some more degrees of freedom.

Let's fine-tune the model to allow more flexibility in duration:

```{r}
m.bam2 <- bam(
  Calories ~ 
    s(Duration, k=12) + 
    s(Heart_Rate, k=12) + 
    s(Body_Temp, k=12),
  data = df.train,
  method = 'fREML' #fast REML
)

summary(m.bam2)

# Print custom evaluation metric
cat('\n')
evaluate(m.bam2)
```

```{r}
# Check for any remaining patterns
gam.check(m.bam2)
```

+ The model reached better k-indexes, but it also worsened validation predictions. Most likely, it is starting to over fit.

# Evaluate Submission Data
```{r}
pred <- predict(m.bam, df.submission)
submission <- data.frame(
  submissions_ids,
  pred
)

# Required header
colnames(submission) <- c('id', 'Calories')

# Save file and show results
write.csv(submission, file = 'output/submission_file.csv', row.names = FALSE)
head(submission)
```

